{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5187a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing an LLM and Setting up Langsmith\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Langchain Tutorial\"\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09f116",
   "metadata": {},
   "source": [
    "# Conversational Memory :\n",
    "> *Conversational Memoery* allows our chatbots and agents to remember precious interactions within a conversation. <br>\n",
    "\n",
    "Type of Conversational Memory\n",
    "1. Conversation Buffer Memory\n",
    "2. Conversation Buffer Window Memory\n",
    "3. Conversation Summary Memory\n",
    "4. Conversation Summary Buffer Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598bc75",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Conversation Buffer Memory\n",
    "\n",
    "*ConversationBufferMemory* is the simplest form of conversational memory, it is literally just a place that we store messages, and then use to feed messages into our LLM. <br> <br> \n",
    "Let's start with LangChain's original ConversationBufferMemory object, we are setting return_messages=True to return the messages as a list of ChatMessage objects — unless using a non-chat model we would always set this to True as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3793617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3307039",
   "metadata": {},
   "source": [
    "There are several ways that we can add messages to our memory, using the save_context method we can add a user query (via the input key) and the AI's response (via the output key). So, to create the following conversation: <br>\n",
    "\n",
    "User: Hi, my name is James <br>\n",
    "AI: Hey James, what's up? I'm an AI model called Zeta. <br>\n",
    "User: I'm researching the different types of conversational memory. <br>\n",
    "AI: That's interesting, what are some examples? <br>\n",
    "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory. <br>\n",
    "AI: That's interesting, what's the difference? <br>\n",
    "User: Buffer memory just stores the entire conversation, right? <br>\n",
    "AI: That makes sense, what about ConversationBufferWindowMemory? <br>\n",
    "User: Buffer window memory stores the last k messages, dropping the rest. <br>\n",
    "AI: Very cool! <br>\n",
    "We do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f23156",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\" : \"Hi, my name is James\"}, #user message\n",
    "    {\"output\" : \"Hey James, what's up? I'm an AI model called Zeta.\"} # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\" : \"I'm researching the different types of conversational memory.\"}, #user message\n",
    "    {\"output\" : \"That's interesting, what are some examples?\"} # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\" : \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"}, #user message\n",
    "    {\"output\" : \"That's interesting, what's the difference?\"} # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\" : \"Buffer memory just stores the entire conversation, right?\"}, #user message\n",
    "    {\"output\" : \"That makes sense, what about ConversationBufferWindowMemory?\"} # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\" : \"Buffer window memory stores the last k messages, dropping the rest.\"}, #user message\n",
    "    {\"output\" : \"Very cool!\"} # AI response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e7b20",
   "metadata": {},
   "source": [
    "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a24f9508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "505d2ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(memory.load_memory_variables({})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064bd38",
   "metadata": {},
   "source": [
    "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the add_user_message and add_ai_message methods. To reproduce what we did above, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c560c30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5e342",
   "metadata": {},
   "source": [
    "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a ConversationChain object — which is already deprecated in favor of the RunnableWithMessageHistory class, which we will cover in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9298de06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankur Singh\\AppData\\Local\\Temp\\ipykernel_11420\\3315403223.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf6516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is My Name Again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is My Name Again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is My Name Again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='You’re James!  (You mentioned that earlier when you introduced yourself.)', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'You’re James!  (You mentioned that earlier when you introduced yourself.)'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is My Name Again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90c009",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory with RunnableWithMessageHistory\n",
    "\n",
    "As mentioned, the ConversationBufferMemory type is due for deprecation. Instead, we can use the RunnableWithMessageHistory class to implement the same functionality.\n",
    "\n",
    "When implementing RunnableWithMessageHistory we will use LangChain Expression Language (LCEL) and for this we need to define our prompt template and LLM components. Our llm has already been defined, so now we just define a ChatPromptTemplate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d6af4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016b3e2",
   "metadata": {},
   "source": [
    "We can link our prompt_template and our llm together to create a pipeline via LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f694ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c97f9",
   "metadata": {},
   "source": [
    "Our RunnableWithMessageHistory requires our pipeline to be wrapped in a RunnableWithMessageHistory object. This object requires a few input parameters. One of those is get_session_history, which requires a function that returns a ChatMessageHistory object based on a session ID. We define this function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92660f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id : str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # If session_id doesn't exists, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc646c",
   "metadata": {},
   "source": [
    "We also need to tell our runnable which variable name to use for the chat history (ie history) and which to use for the user's query (ie query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1311b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history= get_chat_history,\n",
    "    input_messages_key= \"query\",\n",
    "    history_messages_key= \"history\"\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244b383",
   "metadata": {},
   "source": [
    "Now we invoke our Runnable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f59c0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline_with_history.invoke(\n",
    "    {\n",
    "        \"query\" : \"Hi, My name is Ankur and I am GenAI Developer.\"\n",
    "    },\n",
    "    config = {\"session_id\" : \"1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66c0ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nice to meet you again, Ankur! 😊  \\n'\n",
      " 'What’s on your mind today? Are you working on a particular project, need '\n",
      " 'help with a code snippet, or curious about the latest trends in generative '\n",
      " 'AI? Let me know how I can assist!')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34292765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You’re **Ankur**, and you’re a **GenAI Developer**—someone who builds, '\n",
      " 'experiments with, and deploys generative AI models and applications.')\n"
     ]
    }
   ],
   "source": [
    "ans = pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is My name and what do I do?\"},\n",
    "    config = {\"session_id\" : \"1\"}\n",
    ")\n",
    "\n",
    "pprint(ans.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8128b",
   "metadata": {},
   "source": [
    "## 2. Conversation Buffer Window Memory\n",
    "\n",
    "The ConversationBufferWindowMemory type is similar to ConversationBufferMemory, but only keeps track of the last k messages. There are a few reasons why we would want to keep only the last k messages:\n",
    "\n",
    "* More messages mean more tokens are sent with each request, more tokens increases latency and cost.\n",
    "\n",
    "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or \"forget\" information provided to them. Conciseness is key to high performing LLMs.\n",
    "\n",
    "* If we keep all messages we will eventually hit the LLM's context window limit, by adding a window size k we can ensure we never hit this limit.\n",
    "\n",
    "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12fc3f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankur Singh\\AppData\\Local\\Temp\\ipykernel_6760\\3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061794c7",
   "metadata": {},
   "source": [
    "We populate this memory using the same methods as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1622fe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})[\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b831d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankur Singh\\AppData\\Local\\Temp\\ipykernel_6760\\3315403223.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0751c6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my age?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my age?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'I’m sorry, but I don’t know your age.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\":\"what is my age?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38729e4b",
   "metadata": {},
   "source": [
    "The reason our LLM can no longer remember our name is because we have set the k parameter to 4, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves.\n",
    "\n",
    "Based on the agent forgetting our name, we might wonder why we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29864e24",
   "metadata": {},
   "source": [
    "### Conversation Buffer Window Memory with Runnable With Message History\n",
    "To implement this memory type using the RunnableWithMessageHistory class, we can use the same approach as before. We define our prompt_template and llm as before, and then wrap our pipeline in a RunnableWithMessageHistory object.\n",
    "\n",
    "For the window feature, we need to define a custom version of the InMemoryChatMessageHistory class that removes any messages beyond the last k messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f4a0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16257f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2a5519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6ab7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history= get_chat_history,\n",
    "    input_messages_key= \"query\",\n",
    "    history_messages_key= \"history\",\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id= \"session_id\",\n",
    "            annotation= str,\n",
    "            name = \"Session ID\",\n",
    "            description = \"The Session ID to use for the chat history\",\n",
    "            default= \"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"k\",\n",
    "            annotation= int,\n",
    "            name = \"k\",\n",
    "            description = \"The number of messages to keep in the history\",\n",
    "            default = 4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15249bbd",
   "metadata": {},
   "source": [
    "Now we invoke our runnable, this time passing a k parameter via the config parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06f9e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=1 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Ankur and I am a GenAI Developer.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aae9f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Ankur! 👋 Nice to meet a fellow GenAI developer. How can I assist you today? Whether you’re working on a new model, fine‑tuning, deployment, or just looking for some best‑practice tips, I’m here to help.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7753e2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=1 and k=4\n",
      "**Introduction**\n",
      "\n",
      "Hi, I’m Ankur — a GenAI Developer passionate about building intelligent, data‑driven solutions that bridge the gap between cutting‑edge research and real‑world impact. With a solid foundation in machine learning, natural language processing, and cloud‑native deployment, I specialize in designing, fine‑tuning, and scaling generative AI models for a variety of industries—from customer‑service chatbots to creative content generators and beyond.\n",
      "\n",
      "I thrive on tackling complex problems, optimizing model performance, and ensuring that AI systems are both ethical and highly performant. When I’m not writing code or experimenting with the latest transformer architectures, you’ll find me mentoring junior engineers, contributing to open‑source projects, or exploring the newest research papers in generative AI.\n",
      "\n",
      "Let’s connect and explore how we can turn ideas into intelligent, scalable solutions!\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_with_history.invoke(\n",
    "    {\"query\": \"Write an Introduction for me.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\", \"k\": 4}}\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a3ac5",
   "metadata": {},
   "source": [
    "We can also modify the messages that are stored in memory by modifying the records inside the chat_map dictionary directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaceb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "from pprint import pprint   \n",
    "pprint(chat_map[\"id_k4\"].messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c28d7306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don’t have that information stored. Could you tell me your name again?', additional_kwargs={'reasoning_content': 'We have a conversation: user says \"Buffer window memory stores the last k messages, dropping the rest.\" Then assistant says \"Very cool!\" Then user asks \"what is my name again?\" Assistant says \"I’m not sure what you’re called—could you let me know your name?\" Then user repeats: \"what is my name again?\" The assistant hasn\\'t been given any name. The user might be testing memory. According to the system instruction: The assistant should not claim to have memory. We can politely say we don\\'t have that info. We can ask them again. We should not claim to remember. We can say we don\\'t know.'}, response_metadata={'token_usage': {'completion_tokens': 155, 'prompt_tokens': 148, 'total_tokens': 303, 'completion_time': 0.138507529, 'prompt_time': 0.009551516, 'queue_time': 0.052551584, 'total_time': 0.148059045}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a5ac2a5d7b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--23485487-d711-4b64-aa88-734da0fd3989-0', usage_metadata={'input_tokens': 148, 'output_tokens': 155, 'total_tokens': 303})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1217230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='I’m not sure what you’re called—could you let me know your name?', additional_kwargs={'reasoning_content': 'The user asks: \"what is my name again?\" They likely want their name. We need to check if we have the user\\'s name. In prior conversation, the user hasn\\'t given their name. There\\'s no mention. So we should respond that we don\\'t know. According to policy: If we don\\'t know the user\\'s name, we should say we don\\'t know. We should not guess. So we should respond that we don\\'t know.'}, response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 146, 'total_tokens': 258, 'completion_time': 0.103248967, 'prompt_time': 0.013081489, 'queue_time': 3.796570634, 'total_time': 0.116330456}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_ebaf47239f', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a0176968-3023-4cac-b471-39ae1d583430-0', usage_metadata={'input_tokens': 146, 'output_tokens': 112, 'total_tokens': 258}),\n",
      " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='I don’t have that information stored. Could you tell me your name again?', additional_kwargs={'reasoning_content': 'We have a conversation: user says \"Buffer window memory stores the last k messages, dropping the rest.\" Then assistant says \"Very cool!\" Then user asks \"what is my name again?\" Assistant says \"I’m not sure what you’re called—could you let me know your name?\" Then user repeats: \"what is my name again?\" The assistant hasn\\'t been given any name. The user might be testing memory. According to the system instruction: The assistant should not claim to have memory. We can politely say we don\\'t have that info. We can ask them again. We should not claim to remember. We can say we don\\'t know.'}, response_metadata={'token_usage': {'completion_tokens': 155, 'prompt_tokens': 148, 'total_tokens': 303, 'completion_time': 0.138507529, 'prompt_time': 0.009551516, 'queue_time': 0.052551584, 'total_time': 0.148059045}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a5ac2a5d7b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--23485487-d711-4b64-aa88-734da0fd3989-0', usage_metadata={'input_tokens': 148, 'output_tokens': 155, 'total_tokens': 303})]\n"
     ]
    }
   ],
   "source": [
    "pprint(chat_map[\"id_k4\"].messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1eb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
